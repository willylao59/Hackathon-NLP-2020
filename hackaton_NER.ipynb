{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackaton_NER",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEVKY8Q1ImXkdQer80z17Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willylao59/Hackathon_NER/blob/master/hackaton_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BM7wXXZOV3o",
        "colab_type": "code",
        "outputId": "9ea89fa1-1a9a-479c-8603-a33893e7e486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Io_fbSOI14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = \"/content/drive/My Drive/data_hackaton_NER/\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "aircraft    = pd.read_pickle(PATH + 'Aircraft.pkl')\n",
        "text        = pd.read_pickle(PATH + 'text.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da2Dr8z8brXI",
        "colab_type": "code",
        "outputId": "9c726df8-f990-46da-d6ea-d892890e6dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "train_list  = 'APPROX 5 MI NW OF THE MISSION BAY VOR'.split(' ')\n",
        "train_label = ['O', 'aircraft', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
        "test_list   = 'APPROX 5 MI NW OF THE MISSION BAY VOR'.split(' ')\n",
        "\n",
        "train = pd.DataFrame({'word': train_list, 'train_label': train_label})\n",
        "test  = pd.DataFrame({'word':test_list})\n",
        "\n",
        "np.savetxt(PATH + \"train.txt\", train.values, fmt=)\n",
        "np.savetxt(PATH + \"text.txt\", test.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1433\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not str",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-3b11ced02c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"text.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n\u001b[1;32m   1437\u001b[0m                                     \u001b[0;34m\"format specifier ('%s')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m                                     % (str(X.dtype), format))\n\u001b[0m\u001b[1;32m   1439\u001b[0m                 \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Mismatch between array dtype ('object') and format specifier ('%.18e %.18e')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_M9KPHYctNw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDd_jXPnT5iY",
        "colab_type": "code",
        "outputId": "66d5c981-c990-46f1-cd82-e9aa9af16dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "# define columns\n",
        "columns = {0: 'text', 1: 'aircraft'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder = PATH\n",
        "\n",
        "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='train_file.txt',\n",
        "                              test_file='test_file.txt')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 14:06:10,950 Reading data from /content/drive/My Drive/data_hackaton_NER\n",
            "2020-01-22 14:06:10,952 Train: /content/drive/My Drive/data_hackaton_NER/train_file.txt\n",
            "2020-01-22 14:06:10,953 Dev: None\n",
            "2020-01-22 14:06:10,954 Test: /content/drive/My Drive/data_hackaton_NER/test_file.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-pvYKP7fN7e",
        "colab_type": "code",
        "outputId": "fdd11397-06f0-4f4d-a20e-a4c117b7e456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "len(corpus.train)\n",
        "print(corpus.train[0].to_tagged_string('word'))\n",
        "print(corpus.train[1].to_tagged_string('aircraft'))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A B737-300 DURING A RELEASE FROM CONTRACT MAINT INSPECTION, A TECHNICIAN DISCOVERED UNAPPROVED GREEN TAPE WRAPPED ON FLT CTL CABLES LOCATED ABOVE #2 AIR DATA COMPUTER\n",
            "B737 <aircraft> FLC FORGOT TO PUT THE COCKPIT JUMP SEAT DEPLOYED PRIOR TO PUSH BACK DUE TO DISTRACTION OF EXPECTING TO CHECK RIDER CREDENTIALS PRIOR TO TKOF\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2xya_eAfFF_",
        "colab_type": "code",
        "outputId": "12ad0de6-f283-4934-bfac-87d650d59cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flair.datasets import WNUT_17\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "from typing import List\n",
        "\n",
        "# 1. get the corpus\n",
        "print(corpus)\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'aircraft'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary.idx2item)\n",
        "\n",
        "# 4. initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "\n",
        "    WordEmbeddings('glove'),\n",
        "\n",
        "    # comment in this line to use character embeddings\n",
        "    # CharacterEmbeddings(),\n",
        "\n",
        "    # comment in these lines to use flair embeddings\n",
        "    # FlairEmbeddings('news-forward'),\n",
        "    # FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)\n",
        "\n",
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# 7. start training\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=150)\n",
        "\n",
        "# 8. plot weight traces (optional)\n",
        "from flair.visual.training_curves import Plotter\n",
        "plotter = Plotter()\n",
        "plotter.plot_weights('resources/taggers/example-ner/weights.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus: 19559 train + 2173 dev + 5434 test sentences\n",
            "[b'<unk>', b'O', b'aircraft', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 14:10:31,872 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:10:31,873 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('glove')\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\"\n",
            "2020-01-22 14:10:31,873 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:10:31,877 Corpus: \"Corpus: 19559 train + 2173 dev + 5434 test sentences\"\n",
            "2020-01-22 14:10:31,878 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:10:31,880 Parameters:\n",
            "2020-01-22 14:10:31,882  - learning_rate: \"0.1\"\n",
            "2020-01-22 14:10:31,883  - mini_batch_size: \"32\"\n",
            "2020-01-22 14:10:31,884  - patience: \"3\"\n",
            "2020-01-22 14:10:31,888  - anneal_factor: \"0.5\"\n",
            "2020-01-22 14:10:31,889  - max_epochs: \"150\"\n",
            "2020-01-22 14:10:31,890  - shuffle: \"True\"\n",
            "2020-01-22 14:10:31,891  - train_with_dev: \"False\"\n",
            "2020-01-22 14:10:31,892  - batch_growth_annealing: \"False\"\n",
            "2020-01-22 14:10:31,893 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:10:31,894 Model training base path: \"resources/taggers/example-ner\"\n",
            "2020-01-22 14:10:31,896 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:10:31,898 Device: cuda:0\n",
            "2020-01-22 14:10:31,899 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:10:31,901 Embeddings storage mode: cpu\n",
            "2020-01-22 14:10:31,904 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:10:32,068 epoch 1 - iter 0/612 - loss 35.54094696 - samples/sec: 12432.93\n",
            "2020-01-22 14:10:38,416 epoch 1 - iter 61/612 - loss 2.77801942 - samples/sec: 308.24\n",
            "2020-01-22 14:10:44,933 epoch 1 - iter 122/612 - loss 1.88072086 - samples/sec: 300.30\n",
            "2020-01-22 14:10:51,431 epoch 1 - iter 183/612 - loss 1.53375540 - samples/sec: 301.09\n",
            "2020-01-22 14:10:57,796 epoch 1 - iter 244/612 - loss 1.34206316 - samples/sec: 307.45\n",
            "2020-01-22 14:11:04,217 epoch 1 - iter 305/612 - loss 1.22344209 - samples/sec: 304.76\n",
            "2020-01-22 14:11:10,378 epoch 1 - iter 366/612 - loss 1.13040929 - samples/sec: 317.61\n",
            "2020-01-22 14:11:16,622 epoch 1 - iter 427/612 - loss 1.07127754 - samples/sec: 313.46\n",
            "2020-01-22 14:11:22,991 epoch 1 - iter 488/612 - loss 1.01967206 - samples/sec: 307.16\n",
            "2020-01-22 14:11:29,408 epoch 1 - iter 549/612 - loss 0.97346841 - samples/sec: 305.03\n",
            "2020-01-22 14:11:35,814 epoch 1 - iter 610/612 - loss 0.93416945 - samples/sec: 305.45\n",
            "2020-01-22 14:11:35,885 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:11:35,886 EPOCH 1 done: loss 0.9341 - lr 0.1000\n",
            "2020-01-22 14:11:40,276 DEV : loss 0.47506141662597656 - score 0.9212\n",
            "2020-01-22 14:11:40,361 BAD EPOCHS (no improvement): 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-22 14:11:43,465 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:11:43,555 epoch 2 - iter 0/612 - loss 0.31280625 - samples/sec: 22399.36\n",
            "2020-01-22 14:11:48,873 epoch 2 - iter 61/612 - loss 0.55731987 - samples/sec: 368.04\n",
            "2020-01-22 14:11:54,010 epoch 2 - iter 122/612 - loss 0.55258312 - samples/sec: 381.07\n",
            "2020-01-22 14:11:59,208 epoch 2 - iter 183/612 - loss 0.55227934 - samples/sec: 376.62\n",
            "2020-01-22 14:12:04,406 epoch 2 - iter 244/612 - loss 0.55033369 - samples/sec: 376.71\n",
            "2020-01-22 14:12:09,434 epoch 2 - iter 305/612 - loss 0.53893211 - samples/sec: 389.46\n",
            "2020-01-22 14:12:14,704 epoch 2 - iter 366/612 - loss 0.53317045 - samples/sec: 371.65\n",
            "2020-01-22 14:12:19,686 epoch 2 - iter 427/612 - loss 0.52909539 - samples/sec: 393.00\n",
            "2020-01-22 14:12:24,802 epoch 2 - iter 488/612 - loss 0.52362345 - samples/sec: 382.80\n",
            "2020-01-22 14:12:30,002 epoch 2 - iter 549/612 - loss 0.52319922 - samples/sec: 376.56\n",
            "2020-01-22 14:12:35,024 epoch 2 - iter 610/612 - loss 0.52188449 - samples/sec: 389.88\n",
            "2020-01-22 14:12:35,084 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:12:35,085 EPOCH 2 done: loss 0.5216 - lr 0.1000\n",
            "2020-01-22 14:12:38,010 DEV : loss 0.36846575140953064 - score 0.9386\n",
            "2020-01-22 14:12:38,095 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:12:40,815 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:12:40,935 epoch 3 - iter 0/612 - loss 0.27676570 - samples/sec: 16925.73\n",
            "2020-01-22 14:12:46,269 epoch 3 - iter 61/612 - loss 0.54495045 - samples/sec: 367.05\n",
            "2020-01-22 14:12:51,394 epoch 3 - iter 122/612 - loss 0.50071197 - samples/sec: 382.04\n",
            "2020-01-22 14:12:56,397 epoch 3 - iter 183/612 - loss 0.48343082 - samples/sec: 391.29\n",
            "2020-01-22 14:13:01,530 epoch 3 - iter 244/612 - loss 0.47461917 - samples/sec: 381.56\n",
            "2020-01-22 14:13:06,754 epoch 3 - iter 305/612 - loss 0.46450859 - samples/sec: 375.02\n",
            "2020-01-22 14:13:11,912 epoch 3 - iter 366/612 - loss 0.45596829 - samples/sec: 379.53\n",
            "2020-01-22 14:13:16,851 epoch 3 - iter 427/612 - loss 0.45170216 - samples/sec: 396.63\n",
            "2020-01-22 14:13:21,893 epoch 3 - iter 488/612 - loss 0.44174392 - samples/sec: 388.37\n",
            "2020-01-22 14:13:26,932 epoch 3 - iter 549/612 - loss 0.43642422 - samples/sec: 388.46\n",
            "2020-01-22 14:13:31,961 epoch 3 - iter 610/612 - loss 0.43616659 - samples/sec: 389.30\n",
            "2020-01-22 14:13:32,013 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:13:32,014 EPOCH 3 done: loss 0.4355 - lr 0.1000\n",
            "2020-01-22 14:13:35,034 DEV : loss 0.26355960965156555 - score 0.9586\n",
            "2020-01-22 14:13:35,130 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:13:37,877 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:13:37,986 epoch 4 - iter 0/612 - loss 0.69183576 - samples/sec: 19246.39\n",
            "2020-01-22 14:13:43,342 epoch 4 - iter 61/612 - loss 0.40420203 - samples/sec: 365.41\n",
            "2020-01-22 14:13:48,475 epoch 4 - iter 122/612 - loss 0.39542264 - samples/sec: 381.68\n",
            "2020-01-22 14:13:53,449 epoch 4 - iter 183/612 - loss 0.38857726 - samples/sec: 393.60\n",
            "2020-01-22 14:13:58,429 epoch 4 - iter 244/612 - loss 0.37559797 - samples/sec: 393.04\n",
            "2020-01-22 14:14:03,584 epoch 4 - iter 305/612 - loss 0.36847090 - samples/sec: 379.63\n",
            "2020-01-22 14:14:08,654 epoch 4 - iter 366/612 - loss 0.36293935 - samples/sec: 386.16\n",
            "2020-01-22 14:14:13,846 epoch 4 - iter 427/612 - loss 0.36463039 - samples/sec: 377.28\n",
            "2020-01-22 14:14:18,878 epoch 4 - iter 488/612 - loss 0.36934143 - samples/sec: 389.39\n",
            "2020-01-22 14:14:24,005 epoch 4 - iter 549/612 - loss 0.36496540 - samples/sec: 381.98\n",
            "2020-01-22 14:14:29,078 epoch 4 - iter 610/612 - loss 0.36044386 - samples/sec: 385.95\n",
            "2020-01-22 14:14:29,145 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:14:29,146 EPOCH 4 done: loss 0.3601 - lr 0.1000\n",
            "2020-01-22 14:14:32,015 DEV : loss 0.18287354707717896 - score 0.9691\n",
            "2020-01-22 14:14:32,097 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:14:34,856 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:14:34,977 epoch 5 - iter 0/612 - loss 0.46904278 - samples/sec: 16723.11\n",
            "2020-01-22 14:14:39,912 epoch 5 - iter 61/612 - loss 0.31750790 - samples/sec: 396.74\n",
            "2020-01-22 14:14:44,959 epoch 5 - iter 122/612 - loss 0.30571580 - samples/sec: 387.91\n",
            "2020-01-22 14:14:50,149 epoch 5 - iter 183/612 - loss 0.29341834 - samples/sec: 377.22\n",
            "2020-01-22 14:14:55,090 epoch 5 - iter 244/612 - loss 0.29269693 - samples/sec: 396.22\n",
            "2020-01-22 14:15:00,109 epoch 5 - iter 305/612 - loss 0.30043188 - samples/sec: 390.08\n",
            "2020-01-22 14:15:05,401 epoch 5 - iter 366/612 - loss 0.30996206 - samples/sec: 369.90\n",
            "2020-01-22 14:15:10,477 epoch 5 - iter 427/612 - loss 0.30981768 - samples/sec: 385.85\n",
            "2020-01-22 14:15:15,552 epoch 5 - iter 488/612 - loss 0.30600368 - samples/sec: 385.88\n",
            "2020-01-22 14:15:20,559 epoch 5 - iter 549/612 - loss 0.30713899 - samples/sec: 390.98\n",
            "2020-01-22 14:15:25,531 epoch 5 - iter 610/612 - loss 0.31116463 - samples/sec: 393.72\n",
            "2020-01-22 14:15:25,596 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:15:25,597 EPOCH 5 done: loss 0.3109 - lr 0.1000\n",
            "2020-01-22 14:15:28,464 DEV : loss 0.16179221868515015 - score 0.9722\n",
            "2020-01-22 14:15:28,545 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:15:31,194 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:15:31,315 epoch 6 - iter 0/612 - loss 0.11210835 - samples/sec: 16685.11\n",
            "2020-01-22 14:15:36,521 epoch 6 - iter 61/612 - loss 0.29031487 - samples/sec: 376.12\n",
            "2020-01-22 14:15:41,769 epoch 6 - iter 122/612 - loss 0.27861534 - samples/sec: 373.16\n",
            "2020-01-22 14:15:46,959 epoch 6 - iter 183/612 - loss 0.27855861 - samples/sec: 377.17\n",
            "2020-01-22 14:15:51,918 epoch 6 - iter 244/612 - loss 0.28588305 - samples/sec: 394.82\n",
            "2020-01-22 14:15:56,934 epoch 6 - iter 305/612 - loss 0.29250477 - samples/sec: 390.18\n",
            "2020-01-22 14:16:01,898 epoch 6 - iter 366/612 - loss 0.29106315 - samples/sec: 394.63\n",
            "2020-01-22 14:16:06,937 epoch 6 - iter 427/612 - loss 0.29319678 - samples/sec: 388.49\n",
            "2020-01-22 14:16:11,996 epoch 6 - iter 488/612 - loss 0.29129011 - samples/sec: 386.89\n",
            "2020-01-22 14:16:16,932 epoch 6 - iter 549/612 - loss 0.29177180 - samples/sec: 396.65\n",
            "2020-01-22 14:16:21,857 epoch 6 - iter 610/612 - loss 0.29132262 - samples/sec: 397.58\n",
            "2020-01-22 14:16:21,904 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:16:21,905 EPOCH 6 done: loss 0.2909 - lr 0.1000\n",
            "2020-01-22 14:16:24,790 DEV : loss 0.1499415636062622 - score 0.9741\n",
            "2020-01-22 14:16:24,872 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:16:27,539 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:16:27,645 epoch 7 - iter 0/612 - loss 0.24412227 - samples/sec: 19421.57\n",
            "2020-01-22 14:16:32,638 epoch 7 - iter 61/612 - loss 0.29173460 - samples/sec: 392.07\n",
            "2020-01-22 14:16:37,695 epoch 7 - iter 122/612 - loss 0.25597381 - samples/sec: 387.20\n",
            "2020-01-22 14:16:42,798 epoch 7 - iter 183/612 - loss 0.26332002 - samples/sec: 383.71\n",
            "2020-01-22 14:16:47,914 epoch 7 - iter 244/612 - loss 0.26253864 - samples/sec: 382.74\n",
            "2020-01-22 14:16:52,861 epoch 7 - iter 305/612 - loss 0.25906584 - samples/sec: 395.73\n",
            "2020-01-22 14:16:57,931 epoch 7 - iter 366/612 - loss 0.26203529 - samples/sec: 386.02\n",
            "2020-01-22 14:17:03,037 epoch 7 - iter 427/612 - loss 0.26184485 - samples/sec: 383.33\n",
            "2020-01-22 14:17:08,096 epoch 7 - iter 488/612 - loss 0.26710243 - samples/sec: 387.21\n",
            "2020-01-22 14:17:13,120 epoch 7 - iter 549/612 - loss 0.26677742 - samples/sec: 389.99\n",
            "2020-01-22 14:17:18,111 epoch 7 - iter 610/612 - loss 0.26778888 - samples/sec: 392.33\n",
            "2020-01-22 14:17:18,161 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:17:18,162 EPOCH 7 done: loss 0.2676 - lr 0.1000\n",
            "2020-01-22 14:17:21,041 DEV : loss 0.14499123394489288 - score 0.9754\n",
            "2020-01-22 14:17:21,123 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:17:23,776 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:17:23,894 epoch 8 - iter 0/612 - loss 0.37971771 - samples/sec: 17235.22\n",
            "2020-01-22 14:17:28,955 epoch 8 - iter 61/612 - loss 0.26565103 - samples/sec: 386.99\n",
            "2020-01-22 14:17:33,893 epoch 8 - iter 122/612 - loss 0.27336765 - samples/sec: 396.42\n",
            "2020-01-22 14:17:39,034 epoch 8 - iter 183/612 - loss 0.26419796 - samples/sec: 380.76\n",
            "2020-01-22 14:17:44,235 epoch 8 - iter 244/612 - loss 0.25537074 - samples/sec: 376.39\n",
            "2020-01-22 14:17:49,353 epoch 8 - iter 305/612 - loss 0.25584466 - samples/sec: 382.56\n",
            "2020-01-22 14:17:54,299 epoch 8 - iter 366/612 - loss 0.25223760 - samples/sec: 395.89\n",
            "2020-01-22 14:17:59,209 epoch 8 - iter 427/612 - loss 0.25298995 - samples/sec: 398.65\n",
            "2020-01-22 14:18:04,305 epoch 8 - iter 488/612 - loss 0.25238238 - samples/sec: 384.21\n",
            "2020-01-22 14:18:09,193 epoch 8 - iter 549/612 - loss 0.25415756 - samples/sec: 400.69\n",
            "2020-01-22 14:18:14,294 epoch 8 - iter 610/612 - loss 0.25455219 - samples/sec: 383.90\n",
            "2020-01-22 14:18:14,350 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:18:14,351 EPOCH 8 done: loss 0.2544 - lr 0.1000\n",
            "2020-01-22 14:18:17,208 DEV : loss 0.13997918367385864 - score 0.9756\n",
            "2020-01-22 14:18:17,296 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:18:20,008 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:18:20,117 epoch 9 - iter 0/612 - loss 0.18389940 - samples/sec: 18934.46\n",
            "2020-01-22 14:18:25,092 epoch 9 - iter 61/612 - loss 0.26461205 - samples/sec: 393.94\n",
            "2020-01-22 14:18:29,986 epoch 9 - iter 122/612 - loss 0.26760544 - samples/sec: 400.07\n",
            "2020-01-22 14:18:34,981 epoch 9 - iter 183/612 - loss 0.26029019 - samples/sec: 391.90\n",
            "2020-01-22 14:18:39,954 epoch 9 - iter 244/612 - loss 0.25897015 - samples/sec: 393.92\n",
            "2020-01-22 14:18:45,089 epoch 9 - iter 305/612 - loss 0.26405309 - samples/sec: 381.25\n",
            "2020-01-22 14:18:50,392 epoch 9 - iter 366/612 - loss 0.26452876 - samples/sec: 369.06\n",
            "2020-01-22 14:18:55,489 epoch 9 - iter 427/612 - loss 0.25624229 - samples/sec: 384.11\n",
            "2020-01-22 14:19:00,507 epoch 9 - iter 488/612 - loss 0.25303422 - samples/sec: 390.27\n",
            "2020-01-22 14:19:05,632 epoch 9 - iter 549/612 - loss 0.25467671 - samples/sec: 382.06\n",
            "2020-01-22 14:19:10,682 epoch 9 - iter 610/612 - loss 0.25385294 - samples/sec: 387.63\n",
            "2020-01-22 14:19:10,727 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:19:10,728 EPOCH 9 done: loss 0.2535 - lr 0.1000\n",
            "2020-01-22 14:19:13,581 DEV : loss 0.14132164418697357 - score 0.9757\n",
            "2020-01-22 14:19:13,664 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:19:16,299 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:19:16,409 epoch 10 - iter 0/612 - loss 0.10716510 - samples/sec: 18964.68\n",
            "2020-01-22 14:19:21,433 epoch 10 - iter 61/612 - loss 0.27104165 - samples/sec: 389.61\n",
            "2020-01-22 14:19:26,352 epoch 10 - iter 122/612 - loss 0.26338873 - samples/sec: 398.78\n",
            "2020-01-22 14:19:31,311 epoch 10 - iter 183/612 - loss 0.26439630 - samples/sec: 394.74\n",
            "2020-01-22 14:19:36,309 epoch 10 - iter 244/612 - loss 0.26434521 - samples/sec: 391.84\n",
            "2020-01-22 14:19:41,395 epoch 10 - iter 305/612 - loss 0.25549976 - samples/sec: 384.91\n",
            "2020-01-22 14:19:46,582 epoch 10 - iter 366/612 - loss 0.24518080 - samples/sec: 377.55\n",
            "2020-01-22 14:19:51,622 epoch 10 - iter 427/612 - loss 0.24311069 - samples/sec: 388.52\n",
            "2020-01-22 14:19:56,696 epoch 10 - iter 488/612 - loss 0.24146739 - samples/sec: 386.00\n",
            "2020-01-22 14:20:01,688 epoch 10 - iter 549/612 - loss 0.24123780 - samples/sec: 392.12\n",
            "2020-01-22 14:20:06,906 epoch 10 - iter 610/612 - loss 0.24180171 - samples/sec: 375.27\n",
            "2020-01-22 14:20:06,965 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:20:06,966 EPOCH 10 done: loss 0.2416 - lr 0.1000\n",
            "2020-01-22 14:20:09,845 DEV : loss 0.1334342211484909 - score 0.976\n",
            "2020-01-22 14:20:09,939 BAD EPOCHS (no improvement): 0\n",
            "2020-01-22 14:20:12,692 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-22 14:20:12,790 epoch 11 - iter 0/612 - loss 0.24901772 - samples/sec: 20965.45\n",
            "2020-01-22 14:20:17,785 epoch 11 - iter 61/612 - loss 0.24104303 - samples/sec: 391.90\n",
            "2020-01-22 14:20:22,781 epoch 11 - iter 122/612 - loss 0.22942388 - samples/sec: 391.95\n",
            "2020-01-22 14:20:27,773 epoch 11 - iter 183/612 - loss 0.22447262 - samples/sec: 392.23\n",
            "2020-01-22 14:20:32,690 epoch 11 - iter 244/612 - loss 0.22208411 - samples/sec: 398.10\n",
            "2020-01-22 14:20:37,661 epoch 11 - iter 305/612 - loss 0.23352300 - samples/sec: 393.85\n",
            "2020-01-22 14:20:42,830 epoch 11 - iter 366/612 - loss 0.23519407 - samples/sec: 378.80\n",
            "2020-01-22 14:20:48,021 epoch 11 - iter 427/612 - loss 0.23544599 - samples/sec: 377.19\n",
            "2020-01-22 14:20:53,055 epoch 11 - iter 488/612 - loss 0.23060543 - samples/sec: 388.95\n",
            "2020-01-22 14:20:58,062 epoch 11 - iter 549/612 - loss 0.23204291 - samples/sec: 391.05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aTnyMPbfFD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model you trained\n",
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "\n",
        "# create example sentence\n",
        "sentence = Sentence('I love Berlin at A300 et Beluga XL laze layx')\n",
        "\n",
        "# predict tags and print\n",
        "model.predict(sentence)\n",
        "\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV5c0XuWfE_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEYGVyL5fEtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}